{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQHAWIkmK08L"
      },
      "source": [
        "# **Hybrid Search**\n",
        "**BM25** is a sophisticated ranking function used in information retrieval. Acting like a highly efficient librarian, it excels in navigating through extensive collections of documents. Its effectiveness lies in term Frequency: Evaluating how often search terms appear in each document.Vector Search extends our search capabilities beyond mere keyword matching. It brings in a layer of contextual understanding, interpreting the semantics of search queries to provide results that align with the intended meaning\n",
        "\n",
        "**Hybrid Search Approach** - Our hybrid search system synergizes BM25's keyword-focused precision with Vector search's semantic understanding. This duo delivers nuanced, comprehensive search results, perfect for complex and varied datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvgtOorWAKTW"
      },
      "source": [
        "#Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR-w8rVaun_x",
        "outputId": "7793462e-3de1-41d6-ae6c-40a4b632daac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence_transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 sentence_transformers-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2Mu87ReJLQE",
        "outputId": "00adff8a-9ba0-47b0-b6a7-18f5c751a916"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4CuEPJfJxfy",
        "outputId": "fef120ac-9485-476b-eaac-d61fd9fee908"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak2thKdqDCAU",
        "outputId": "e8d11e33-d124-49cd-eb2d-eb85c820b6c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.26.4)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF7a1Ae9I2nU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "file_path = '/content/drive/MyDrive/zepto DS/processed.csv'\n",
        "data = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "nj___q8mI4ht",
        "outputId": "fd54010e-b19f-4438-d517-25dbdc3fe175"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>uniq_id</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>crawl_timestamp</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product_url</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product_name</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product_category_tree</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pid</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>retail_price</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>discounted_price</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>image</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is_FK_Advantage_product</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>description</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product_rating</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>overall_rating</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>brand</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>product_specifications</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>parsed_category_tree</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top_level_category</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "uniq_id                    0\n",
              "crawl_timestamp            0\n",
              "product_url                0\n",
              "product_name               0\n",
              "product_category_tree      0\n",
              "pid                        0\n",
              "retail_price               0\n",
              "discounted_price           0\n",
              "image                      0\n",
              "is_FK_Advantage_product    0\n",
              "description                0\n",
              "product_rating             0\n",
              "overall_rating             0\n",
              "brand                      0\n",
              "product_specifications     0\n",
              "parsed_category_tree       0\n",
              "top_level_category         0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZrWNhfWDOv8"
      },
      "source": [
        "#**KEY WORD SEARCH USING BM25**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYP_y69HKGbZ",
        "outputId": "13d2f091-74a3-45fc-b8b3-a4cd13ebbe03"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing texts: 100%|██████████| 19995/19995 [06:18<00:00, 52.82it/s] \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load data\n",
        "file_path = '/content/drive/MyDrive/zepto DS/processed.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    tokens = word_tokenize(text)  # Tokenize\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]  # Stemming\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing to the product_name and description with tqdm\n",
        "tqdm.pandas(desc=\"Processing texts\")\n",
        "data['processed_text'] = data.progress_apply(\n",
        "    lambda row: preprocess_text(f\"{row['product_name']} {row['description']} {row['brand']} {row['top_level_category']}\"),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Indexing\n",
        "corpus = data['processed_text'].tolist()\n",
        "bm25 = BM25Okapi(corpus)\n",
        "\n",
        "# Search Function\n",
        "def search(query, bm25, data, top_n=10):\n",
        "    query = preprocess_text(query)\n",
        "    scores = bm25.get_scores(query)\n",
        "    top_n_indices = np.argsort(scores)[::-1][:top_n]\n",
        "    return data.iloc[top_n_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtklRPyBDTNp"
      },
      "source": [
        "#**SEARCH USING BM25**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7zM8ufwLent",
        "outputId": "b76302fc-fc1a-436b-eb8a-bc1a628921e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                    product_name  \\\n",
            "7            FabHomeDecor Fabric Double Sofa Bed   \n",
            "16           FabHomeDecor Fabric Double Sofa Bed   \n",
            "1            FabHomeDecor Fabric Double Sofa Bed   \n",
            "19           FabHomeDecor Fabric Double Sofa Bed   \n",
            "11494              ARRA Solid Wood 2 Seater Sofa   \n",
            "11346              ARRA Solid Wood 3 Seater Sofa   \n",
            "11336              ARRA Solid Wood 4 Seater Sofa   \n",
            "11662              ARRA Solid Wood 4 Seater Sofa   \n",
            "14839     Fashion Centre Double Bed Mosquito Net   \n",
            "18115  Unnati Floral Double Top Sheet Multicolor   \n",
            "\n",
            "                                             description  discounted_price  \n",
            "7      FabHomeDecor Fabric Double Sofa Bed (Finish Co...           22646.0  \n",
            "16     FabHomeDecor Fabric Double Sofa Bed (Finish Co...           22646.0  \n",
            "1      FabHomeDecor Fabric Double Sofa Bed (Finish Co...           22646.0  \n",
            "19     FabHomeDecor Fabric Double Sofa Bed (Finish Co...           22646.0  \n",
            "11494  ARRA Solid Wood 2 Seater Sofa (Finish Color - ...           19900.0  \n",
            "11346  ARRA Solid Wood 3 Seater Sofa (Finish Color - ...           14500.0  \n",
            "11336  ARRA Solid Wood 4 Seater Sofa (Finish Color - ...           25800.0  \n",
            "11662  ARRA Solid Wood 4 Seater Sofa (Finish Color - ...           43100.0  \n",
            "14839  Buy Fashion Centre Double Bed Mosquito Net for...             960.0  \n",
            "18115  Unnati Floral Double Top Sheet Multicolor (Bla...             999.0  \n"
          ]
        }
      ],
      "source": [
        "query = \"Fabric Double Sofa Bed\"\n",
        "results = search(query, bm25, data)\n",
        "\n",
        "# Display the top 10 results\n",
        "print(results[['product_name', 'description', 'discounted_price']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdJcK7ZyDa-4"
      },
      "source": [
        "#**BM25 + SEMTANTIC SEARCH**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDJmPeyDLxk2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load data\n",
        "file_path = '/content/drive/MyDrive/zepto DS/processed.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    return text\n",
        "\n",
        "# Combine text fields and preprocess\n",
        "data['combined_text'] = data.apply(lambda row: preprocess_text(f\"{row['product_name']} {row['description']} {row['brand']} {row['top_level_category']}\"), axis=1)\n",
        "\n",
        "# Load Sentence Transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for the combined text\n",
        "tqdm.pandas(desc=\"Generating embeddings\")\n",
        "#data['embedding'] = data['combined_text'].progress_apply(lambda x: model.encode(x))\n",
        "embeddings_df = pd.read_csv('/content/drive/MyDrive/zepto DS/embeddings.csv')\n",
        "\n",
        "# Convert the DataFrame to a list of embeddings\n",
        "# Assuming each row in the CSV represents an embedding vector\n",
        "embeddings = embeddings_df.values.tolist()\n",
        "\n",
        "# Convert the list of lists into a NumPy array\n",
        "embeddings_array = np.array(embeddings)\n",
        "# Initialize BM25\n",
        "corpus = data['combined_text'].tolist()\n",
        "bm25 = BM25Okapi([text.split() for text in corpus])  # BM25 requires tokenized texts\n",
        "\n",
        "# Search Function\n",
        "def search(query, bm25, model, data, top_n=10):\n",
        "    # Preprocess and generate embedding for the query\n",
        "    query = preprocess_text(query)\n",
        "    query_embedding = model.encode(query)\n",
        "\n",
        "    # Compute BM25 scores\n",
        "    query_tokens = query.split()\n",
        "    bm25_scores = bm25.get_scores(query_tokens)\n",
        "\n",
        "    # Compute vector similarity scores\n",
        "    query_vector = model.encode(query)\n",
        "    embeddings = np.vstack(embeddings_array)\n",
        "    vector_scores = cosine_similarity([query_vector], embeddings)[0]\n",
        "\n",
        "    # Normalize scores\n",
        "    bm25_scores = np.array(bm25_scores)\n",
        "    vector_scores = np.array(vector_scores)\n",
        "\n",
        "    # Combine scores\n",
        "    combined_scores = 0.5 * bm25_scores + 0.5 * vector_scores  # Adjust weights as needed\n",
        "\n",
        "    # Get indices of top_n results\n",
        "    top_n_indices = combined_scores.argsort()[::-1][:top_n]\n",
        "\n",
        "    return data.iloc[top_n_indices]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mdgcw_jDmrg"
      },
      "source": [
        "#SEARCH USING THE HYBRID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyNmOEH2Sv8_",
        "outputId": "94278bec-68c3-476e-fd82-439fea9c11b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                             product_name  \\\n",
            "21    Alisha Solid Women's Cycling Shorts   \n",
            "3     Alisha Solid Women's Cycling Shorts   \n",
            "13    Alisha Solid Women's Cycling Shorts   \n",
            "15    Alisha Solid Women's Cycling Shorts   \n",
            "0     Alisha Solid Women's Cycling Shorts   \n",
            "9     Alisha Solid Women's Cycling Shorts   \n",
            "6     Alisha Solid Women's Cycling Shorts   \n",
            "3417      Roha Collections Womens Loafers   \n",
            "3372       Urban Monkey Womens Pu Loafers   \n",
            "4454         Womens Trendz Alloy Necklace   \n",
            "\n",
            "                                            description  discounted_price  \n",
            "21    Alisha Solid Women's Cycling Shorts - Buy Blac...       1973.401767  \n",
            "3     Key Features of Alisha Solid Women's Cycling S...        267.000000  \n",
            "13    Key Features of Alisha Solid Women's Cycling S...        379.000000  \n",
            "15    Key Features of Alisha Solid Women's Cycling S...        379.000000  \n",
            "0     Key Features of Alisha Solid Women's Cycling S...        379.000000  \n",
            "9     Key Features of Alisha Solid Women's Cycling S...        479.000000  \n",
            "6     Key Features of Alisha Solid Women's Cycling S...        479.000000  \n",
            "3417  Roha Collections Womens Loafers - Buy Roha Col...        599.000000  \n",
            "3372  Urban Monkey Womens Pu Loafers - Buy Urban Mon...       1050.000000  \n",
            "4454  Womens Trendz Alloy Necklace - Buy Womens Tren...        445.000000  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "query = \"alisha womens footwear\"\n",
        "results = search(query, bm25, model, data)\n",
        "\n",
        "# Display the top 10 results\n",
        "print(results[['product_name', 'description', 'discounted_price']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaBLomRNDtuJ"
      },
      "source": [
        "#**BM25 + SEMANTIC SEARCH + HEURISTIC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcRyPVUATjmM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load data\n",
        "file_path = '/content/drive/MyDrive/zepto DS/processed.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Initialize Sentence Transformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    return text\n",
        "\n",
        "# Combine text fields and preprocess\n",
        "data['combined_text'] = data.apply(lambda row: preprocess_text(f\"{row['product_name']} {row['description']} {row['brand']} {row['top_level_category']}\"), axis=1)\n",
        "\n",
        "# Compute embeddings\n",
        "#data['embedding'] = data['combined_text'].progress_apply(lambda x: model.encode(x))\n",
        "embeddings_df = pd.read_csv('/content/drive/MyDrive/zepto DS/embeddings.csv')\n",
        "\n",
        "# Convert the DataFrame to a list of embeddings\n",
        "# Assuming each row in the CSV represents an embedding vector\n",
        "embeddings = embeddings_df.values.tolist()\n",
        "\n",
        "# Convert the list of lists into a NumPy array\n",
        "embeddings_array = np.array(embeddings)\n",
        "# Initialize BM25\n",
        "tokenized_corpus = [text.split() for text in data['combined_text']]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# Function to compute relevance scores\n",
        "def compute_relevance_scores(query, query_embedding, data):\n",
        "    # Preprocess and tokenize query\n",
        "    query_processed = preprocess_text(query)\n",
        "    query_tokens = query_processed.split()\n",
        "\n",
        "    # Compute BM25 scores\n",
        "    bm25_scores = bm25.get_scores(query_tokens)\n",
        "\n",
        "    # Compute TF-IDF scores\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    X_tfidf = tfidf_vectorizer.fit_transform(data['combined_text'])\n",
        "    query_tfidf = tfidf_vectorizer.transform([query])\n",
        "    tfidf_scores = X_tfidf.dot(query_tfidf.T).toarray().flatten()\n",
        "\n",
        "    # Compute cosine similarity scores\n",
        "    cosine_scores = cosine_similarity(query_embedding.reshape(1, -1), embeddings_array).flatten()\n",
        "\n",
        "    # Combine scores\n",
        "    weights = {'bm25': 0.35, 'tfidf': 0.3, 'cosine': 0.35}\n",
        "    combined_scores = (weights['bm25'] * bm25_scores +\n",
        "                       weights['tfidf'] * tfidf_scores +\n",
        "                       weights['cosine'] * cosine_scores)\n",
        "\n",
        "    return combined_scores\n",
        "# Search function\n",
        "def search(query, data, top_n=10):\n",
        "    # Compute query embedding\n",
        "    query_embedding = model.encode(preprocess_text(query))\n",
        "\n",
        "    # Compute relevance scores\n",
        "    scores = compute_relevance_scores(query, query_embedding, data)\n",
        "\n",
        "    # Get top N results\n",
        "    top_n_indices = np.argsort(scores)[::-1][:top_n]\n",
        "    return data.iloc[top_n_indices]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0pN0SJZD4_O"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kql-tvw5D_Bg"
      },
      "source": [
        "#**SEARCH USING HUERISTIC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2Qck-fyD5wY",
        "outputId": "23718687-b659-4f62-bd9e-07d3b25c7fe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                             product_name  \\\n",
            "21    Alisha Solid Women's Cycling Shorts   \n",
            "3     Alisha Solid Women's Cycling Shorts   \n",
            "13    Alisha Solid Women's Cycling Shorts   \n",
            "15    Alisha Solid Women's Cycling Shorts   \n",
            "0     Alisha Solid Women's Cycling Shorts   \n",
            "9     Alisha Solid Women's Cycling Shorts   \n",
            "6     Alisha Solid Women's Cycling Shorts   \n",
            "3417      Roha Collections Womens Loafers   \n",
            "3372       Urban Monkey Womens Pu Loafers   \n",
            "4454         Womens Trendz Alloy Necklace   \n",
            "\n",
            "                                            description  discounted_price  \n",
            "21    Alisha Solid Women's Cycling Shorts - Buy Blac...       1973.401767  \n",
            "3     Key Features of Alisha Solid Women's Cycling S...        267.000000  \n",
            "13    Key Features of Alisha Solid Women's Cycling S...        379.000000  \n",
            "15    Key Features of Alisha Solid Women's Cycling S...        379.000000  \n",
            "0     Key Features of Alisha Solid Women's Cycling S...        379.000000  \n",
            "9     Key Features of Alisha Solid Women's Cycling S...        479.000000  \n",
            "6     Key Features of Alisha Solid Women's Cycling S...        479.000000  \n",
            "3417  Roha Collections Womens Loafers - Buy Roha Col...        599.000000  \n",
            "3372  Urban Monkey Womens Pu Loafers - Buy Urban Mon...       1050.000000  \n",
            "4454  Womens Trendz Alloy Necklace - Buy Womens Tren...        445.000000  \n"
          ]
        }
      ],
      "source": [
        "query = \"alisha womens footwear\"\n",
        "results = search(query, data)\n",
        "\n",
        "# Display the top 10 results\n",
        "print(results[['product_name', 'description', 'discounted_price']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jsWCexxZlpL"
      },
      "outputs": [],
      "source": [
        "#embeddings_df = pd.DataFrame(data['embedding'].tolist())\n",
        "\n",
        "# Save DataFrame to a CSV file\n",
        "#embeddings_df.to_csv('embeddings.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM7-IbosEER1"
      },
      "source": [
        "#**EVALUATION METRICS / RELEVANCE SCORES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m0Oo1vuEWI3"
      },
      "source": [
        "#PRECISION@K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnZDg55HZq-H",
        "outputId": "d110832f-9552-49ca-9d57-b81a1a0c14d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 results for query 'furniture for pets':\n",
            "1. Jerry's Jppb11552 M Pet Bed - BM25 Score: 16.908848995561268, Cosine Similarity: 0.5272480845451355\n",
            "2. Jerry's Jppb11592 M Pet Bed - BM25 Score: 15.718117997867587, Cosine Similarity: 0.5517610311508179\n",
            "3. Jerry's Jppb11584 XL Pet Bed - BM25 Score: 15.669032587171094, Cosine Similarity: 0.5451546907424927\n",
            "4. BM WOOD FURNITURE Hexagon Wall Shelves MDF Wall Shelf - BM25 Score: 10.876524516584913, Cosine Similarity: 0.388668417930603\n",
            "5. Surbhi Bunny  - 60 cm - BM25 Score: 10.690180299819243, Cosine Similarity: 0.31556057929992676\n",
            "6. Hunter Electric Insect Killer - BM25 Score: 10.323929778650841, Cosine Similarity: 0.18109910190105438\n",
            "7. Petshop7 PS7DB0065 M Pet Bed - BM25 Score: 8.86622805636226, Cosine Similarity: 0.624779224395752\n",
            "8. Petshop7 PS7BED000429 M Pet Bed - BM25 Score: 8.86622805636226, Cosine Similarity: 0.6099498271942139\n",
            "9. Petshop7 PS7DB0066 S Pet Bed - BM25 Score: 8.86622805636226, Cosine Similarity: 0.6082963347434998\n",
            "10. Petshop7 PS7DB0040 L Pet Bed - BM25 Score: 8.86622805636226, Cosine Similarity: 0.6074292659759521\n",
            "Precision at 10: 0.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Load data\n",
        "file_path = '/content/drive/MyDrive/zepto DS/processed.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Initialize Sentence Transformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    return text\n",
        "\n",
        "# Combine text fields and preprocess\n",
        "data['combined_text'] = data.apply(lambda row: preprocess_text(f\"{row['product_name']} {row['description']} {row['brand']} {row['top_level_category']}\"), axis=1)\n",
        "\n",
        "# Compute embeddings\n",
        "#data['embedding'] = data['combined_text'].apply(lambda x: model.encode(x))\n",
        "embeddings_df = pd.read_csv('/content/drive/MyDrive/zepto DS/embeddings.csv')\n",
        "\n",
        "# Convert the DataFrame to a list of embeddings\n",
        "# Assuming each row in the CSV represents an embedding vector\n",
        "embeddings = embeddings_df.values.tolist()\n",
        "\n",
        "# Convert the list of lists into a NumPy array\n",
        "embeddings_array = np.array(embeddings)\n",
        "# Initialize BM25\n",
        "tokenized_corpus = [text.split() for text in data['combined_text']]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# Function to compute relevance scores\n",
        "def compute_relevance_scores(query, query_embedding, data):\n",
        "    # Preprocess and tokenize query\n",
        "    query_processed = preprocess_text(query)\n",
        "    query_tokens = query_processed.split()\n",
        "\n",
        "    # Compute BM25 scores\n",
        "    bm25_scores = bm25.get_scores(query_tokens)\n",
        "\n",
        "    # Compute TF-IDF scores\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    X_tfidf = tfidf_vectorizer.fit_transform(data['combined_text'])\n",
        "    query_tfidf = tfidf_vectorizer.transform([query])\n",
        "    tfidf_scores = X_tfidf.dot(query_tfidf.T).toarray().flatten()\n",
        "\n",
        "    # Compute cosine similarity scores\n",
        "    cosine_scores = cosine_similarity(query_embedding.reshape(1, -1), embeddings_array).flatten()\n",
        "\n",
        "    # Combine scores\n",
        "    weights = {'bm25': 0.4, 'tfidf': 0.3, 'cosine': 0.3}\n",
        "    combined_scores = (weights['bm25'] * bm25_scores +\n",
        "                       weights['tfidf'] * tfidf_scores +\n",
        "                       weights['cosine'] * cosine_scores)\n",
        "\n",
        "    return combined_scores\n",
        "\n",
        "# Search function\n",
        "def search(query, data, model, top_n=10):\n",
        "    # Compute query embedding\n",
        "    query_embedding = model.encode(preprocess_text(query))\n",
        "\n",
        "    # Compute relevance scores\n",
        "    scores = compute_relevance_scores(query, query_embedding, data)\n",
        "\n",
        "    # Get top N results\n",
        "    top_n_indices = np.argsort(scores)[::-1][:top_n]\n",
        "    return data.iloc[top_n_indices]\n",
        "\n",
        "# Function to compute Precision at K\n",
        "def precision_at_k(retrieved_docs, query, k):\n",
        "    relevant_docs = [doc for doc in retrieved_docs[:k] if query in doc['combined_text']]\n",
        "    return len(relevant_docs) / k\n",
        "\n",
        "# Function to compute query-document similarity\n",
        "def query_document_similarity(query, documents, model):\n",
        "    query_embedding = model.encode(preprocess_text(query))\n",
        "    doc_embeddings = [model.encode(preprocess_text(doc)) for doc in documents]\n",
        "    similarities = cosine_similarity([query_embedding], doc_embeddings).flatten()\n",
        "    return similarities\n",
        "\n",
        "# Evaluate search approaches\n",
        "def evaluate_search(query, data, model, top_n=10):\n",
        "    # Retrieve search results\n",
        "    results = search(query, data, model, top_n=top_n)\n",
        "\n",
        "    # Compute similarity scores\n",
        "    query_embedding = model.encode(preprocess_text(query))\n",
        "    doc_embeddings = [model.encode(preprocess_text(doc)) for doc in results['combined_text']]\n",
        "    cosine_similarities = cosine_similarity([query_embedding], doc_embeddings).flatten()\n",
        "\n",
        "    # Compute BM25 scores\n",
        "    query_tokens = preprocess_text(query).split()\n",
        "    bm25_scores = bm25.get_scores(query_tokens)\n",
        "\n",
        "    # Precision at K\n",
        "    precision_at_k_value = precision_at_k(results.to_dict('records'), query, top_n)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"Top {top_n} results for query '{query}':\")\n",
        "    for i, (index, row) in enumerate(results.iterrows()):\n",
        "        print(f\"{i+1}. {row['product_name']} - BM25 Score: {bm25_scores[index]}, Cosine Similarity: {cosine_similarities[i]}\")\n",
        "\n",
        "    print(f'Precision at {top_n}: {precision_at_k_value}')\n",
        "\n",
        "query = \"furniture for pets\"\n",
        "evaluate_search(query, data, model, top_n=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6QFwotu-75D",
        "outputId": "0ddae188-14c1-48ae-aac8-2a0c54eb64ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbKphDppEn2X"
      },
      "source": [
        "#**NCDG SCORES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_JvgLzb_NVd",
        "outputId": "df536856-b7b4-46f6-b6e8-2d31c250296d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document 1: Alisha Solid Women's Cycling Shorts | Score: 6.6812\n",
            "Document 2: Alisha Solid Women's Cycling Shorts | Score: 6.0987\n",
            "Document 3: Alisha Solid Women's Cycling Shorts | Score: 6.0895\n",
            "Document 4: Alisha Solid Women's Cycling Shorts | Score: 6.0869\n",
            "Document 5: Alisha Solid Women's Cycling Shorts | Score: 6.0837\n",
            "Document 6: Alisha Solid Women's Cycling Shorts | Score: 6.0719\n",
            "Document 7: Alisha Solid Women's Cycling Shorts | Score: 6.0669\n",
            "Document 8: Roha Collections Womens Loafers | Score: 6.0222\n",
            "Document 9: Urban Monkey Womens Pu Loafers | Score: 5.8927\n",
            "Document 10: Womens Trendz Alloy Necklace | Score: 4.7952\n",
            "Document 11: Womens Trendz Alloy Necklace | Score: 4.7934\n",
            "Document 12: Womens Trendz Alloy Necklace | Score: 4.7934\n",
            "Document 13: Womens Trendz Vertical Ball Thushi Alloy Necklace | Score: 4.6562\n",
            "Document 14: Digni Boots | Score: 4.6020\n",
            "Document 15: Womens Trendz Kolhapuri Saaj Thushi Crystal Yellow Gold Plated Alloy Necklace | Score: 4.5025\n",
            "Document 16: Womens Trendz Kolhapuri Saaj Thushi Crystal Yellow Gold Plated Alloy Necklace | Score: 4.5003\n",
            "Document 17: Womens Trendz Kolhapuri Saaj Thushi Crystal Yellow Gold Plated Alloy Necklace | Score: 4.5003\n",
            "Document 18: Womens Trendz Kolhapuri Saaj Thushi Crystal Yellow Gold Plated Alloy Necklace | Score: 4.5003\n",
            "Document 19: Womens Trendz Kolhapuri Saaj Thushi Crystal Yellow Gold Plated Alloy Necklace | Score: 4.4994\n",
            "Document 20: Womens Trendz Kolhapuri Saaj Thushi Crystal Yellow Gold Plated Alloy Necklace | Score: 4.4994\n",
            "Document 21: Womens Trendz Kolhapuri Saaj Thushi Crystal Yellow Gold Plated Alloy Necklace | Score: 4.4994\n",
            "Document 22: Womens Trendz Half Jhaler Panadi Thushi Crystal Yellow Gold Plated Alloy Necklace | Score: 4.4562\n",
            "Document 23: Womens Trendz Fancy Crystal Mani with Single Pendants Mangalsutra Crystal Yellow Gold Plated Alloy Necklace | Score: 4.3609\n",
            "Document 24: Bombay High Women's Solid Casual Shirt | Score: 4.2278\n",
            "Document 25: Etti Women's Solid Casual Shirt | Score: 4.1918\n",
            "Document 26: Younky Fashion Women's T-Shirt Bra | Score: 4.1881\n",
            "Document 27: Younky Fashion Women's Full Coverage Bra | Score: 4.1840\n",
            "Document 28: Younky Fashion Women's Full Coverage Bra | Score: 4.1840\n",
            "Document 29: Younky Fashion Women's Full Coverage Bra | Score: 4.1828\n",
            "Document 30: Younky Fashion Women's Full Coverage Bra | Score: 4.1811\n",
            "\n",
            "Overall NDCG for top 30 results: 1.0000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "# Load data\n",
        "file_path = '/content/drive/MyDrive/zepto DS/processed.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Initialize Sentence Transformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    return text\n",
        "\n",
        "# Combine text fields and preprocess\n",
        "data['combined_text'] = data.apply(lambda row: preprocess_text(f\"{row['product_name']} {row['description']} {row['brand']} {row['top_level_category']}\"), axis=1)\n",
        "\n",
        "# Load precomputed embeddings\n",
        "embeddings_df = pd.read_csv('/content/drive/MyDrive/zepto DS/embeddings.csv')\n",
        "embeddings_array = np.array(embeddings_df.values.tolist())\n",
        "\n",
        "# Initialize BM25\n",
        "tokenized_corpus = [text.split() for text in data['combined_text']]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# Function to compute relevance scores\n",
        "def compute_relevance_scores(query, query_embedding, data):\n",
        "    # Preprocess and tokenize query\n",
        "    query_processed = preprocess_text(query)\n",
        "    query_tokens = query_processed.split()\n",
        "\n",
        "    # Compute BM25 scores\n",
        "    bm25_scores = bm25.get_scores(query_tokens)\n",
        "\n",
        "    # Compute TF-IDF scores\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    X_tfidf = tfidf_vectorizer.fit_transform(data['combined_text'])\n",
        "    query_tfidf = tfidf_vectorizer.transform([query])\n",
        "    tfidf_scores = X_tfidf.dot(query_tfidf.T).toarray().flatten()\n",
        "\n",
        "    # Compute cosine similarity scores\n",
        "    cosine_scores = cosine_similarity(query_embedding.reshape(1, -1), embeddings_array).flatten()\n",
        "\n",
        "    # Combine scores\n",
        "    weights = {'bm25': 0.4, 'tfidf': 0.3, 'cosine': 0.3}\n",
        "    combined_scores = (weights['bm25'] * bm25_scores +\n",
        "                       weights['tfidf'] * tfidf_scores +\n",
        "                       weights['cosine'] * cosine_scores)\n",
        "\n",
        "    return combined_scores\n",
        "\n",
        "# Function to compute and print NDCG for each retrieved document\n",
        "def ndcg_per_document(query, data, model, top_n=10):\n",
        "    # Retrieve search results\n",
        "    query_embedding = model.encode(preprocess_text(query))\n",
        "    scores = compute_relevance_scores(query, query_embedding, data)\n",
        "    top_n_indices = np.argsort(scores)[::-1][:top_n]\n",
        "\n",
        "    # Generate relevance scores for top N results\n",
        "    relevance_scores = np.array([scores[i] for i in top_n_indices])\n",
        "\n",
        "    # Assume a perfect ranking for true relevance (for demonstration purposes)\n",
        "    true_relevance = np.sort(relevance_scores)[::-1]\n",
        "\n",
        "    # Compute NDCG for the top N results\n",
        "    ndcg = ndcg_score([true_relevance], [relevance_scores], k=top_n)\n",
        "\n",
        "    # Print results\n",
        "    for i, index in enumerate(top_n_indices):\n",
        "        print(f\"Document {i+1}: {data.iloc[index]['product_name']} | Score: {relevance_scores[i]:.4f}\")\n",
        "\n",
        "    print(f\"\\nOverall NDCG for top {top_n} results: {ndcg:.4f}\")\n",
        "\n",
        "\n",
        "query = \"alisha footwear womens\"\n",
        "ndcg_per_document(query, data, model, top_n=30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qrFsRogM0E_",
        "outputId": "42dede4f-574f-417f-cde1-aea192d7fa3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.37.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.4.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Collecting tenacity<9,>=8.1.0 (from streamlit)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<5,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl.metadata (37 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.7.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.37.1-py2.py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: watchdog, tenacity, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.37.1 tenacity-8.5.0 watchdog-4.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxb1qYEgRsye"
      },
      "source": [
        "#**STREAM LIT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJsCSsYMM3rS",
        "outputId": "31f789e9-04da-4231-d90b-6b29819ca48a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "\n",
        "# Load data\n",
        "file_path = '/content/drive/MyDrive/zepto DS/processed.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Initialize Sentence Transformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    return text\n",
        "\n",
        "# Combine text fields and preprocess\n",
        "data['combined_text'] = data.apply(lambda row: preprocess_text(f\"{row['product_name']} {row['description']} {row['brand']} {row['top_level_category']}\"), axis=1)\n",
        "\n",
        "# Load precomputed embeddings\n",
        "embeddings_df = pd.read_csv('/content/drive/MyDrive/zepto DS/embeddings.csv')\n",
        "embeddings = embeddings_df.values.tolist()\n",
        "embeddings_array = np.array(embeddings)\n",
        "\n",
        "# Initialize BM25\n",
        "tokenized_corpus = [text.split() for text in data['combined_text']]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# Function to compute relevance scores\n",
        "def compute_relevance_scores(query, query_embedding, data):\n",
        "    query_processed = preprocess_text(query)\n",
        "    query_tokens = query_processed.split()\n",
        "    bm25_scores = bm25.get_scores(query_tokens)\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    X_tfidf = tfidf_vectorizer.fit_transform(data['combined_text'])\n",
        "    query_tfidf = tfidf_vectorizer.transform([query])\n",
        "    tfidf_scores = X_tfidf.dot(query_tfidf.T).toarray().flatten()\n",
        "    cosine_scores = cosine_similarity(query_embedding.reshape(1, -1), embeddings_array).flatten()\n",
        "    weights = {'bm25': 0.35, 'tfidf': 0.3, 'cosine': 0.35}\n",
        "    combined_scores = (weights['bm25'] * bm25_scores +\n",
        "                       weights['tfidf'] * tfidf_scores +\n",
        "                       weights['cosine'] * cosine_scores)\n",
        "    return combined_scores\n",
        "\n",
        "# Search function\n",
        "def search(query, data, top_n=10):\n",
        "    query_embedding = model.encode(preprocess_text(query))\n",
        "    scores = compute_relevance_scores(query, query_embedding, data)\n",
        "    top_n_indices = np.argsort(scores)[::-1][:top_n]\n",
        "    return data.iloc[top_n_indices]\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"Product Search\")\n",
        "\n",
        "query = st.text_input(\"Enter search query:\")\n",
        "if query:\n",
        "    results = search(query, data)\n",
        "    st.write(\"Top 10 Results:\")\n",
        "    st.dataframe(results[['product_name', 'description', 'discounted_price']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bvxNiyBNGGK",
        "outputId": "8f2a9d41-a4c8-419b-ad5a-02277cfede66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.172.17.171\n"
          ]
        }
      ],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fExj88OcOFh-",
        "outputId": "14950cbc-d248-4b60-cd35-2a0176cc33d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8502\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8502\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.172.17.171:8502\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://sixty-papers-lie.loca.lt\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_2hPU8GQHwJ",
        "outputId": "e7a1f2a1-5c48-4b24-9ae3-0432d1b061b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h\n",
            "added 22 packages, and audited 23 packages in 2s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues, run:\n",
            "  npm audit fix\n",
            "\n",
            "Run `npm audit` for details.\n"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eex016BfQOFC"
      },
      "outputs": [],
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxyCO2b1QRCT",
        "outputId": "eef841ed-56c5-4cbc-b7c2-a3d62b44bed1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "your url is: https://cold-doodles-dream.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
